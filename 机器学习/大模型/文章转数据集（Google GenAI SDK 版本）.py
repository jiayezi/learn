import json
import os
import time
from random import randint
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import requests
from bs4 import BeautifulSoup
import pymysql
from google import genai
from google.genai import types

with open('config.json') as f:
    cfg = json.load(f)

# å…¨å±€å‚æ•°
category_name = 'æ–‡åŒ–'  # åˆ†ç±»åç§°
CHUNK_SIZE = 800  # æ¯æ®µæœ€å¤š 800 å­—
SLEEP_TIME = 1    # æ¯ç¯‡æ–‡ç« ä¹‹é—´ä¼‘çœ æ—¶é—´
original_urls_file = f'output/original_urls {category_name}.txt'
processed_urls_file = f"output/processed_urls {category_name}.txt"  # å·²å¤„ç†çš„ç½‘å€åˆ—è¡¨

# APIå’Œè¾“å‡ºæ–‡ä»¶é…ç½®
base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
api_key = cfg['Gemini_API_KEY']
model_name="gemini-2.5-flash"

output_file = f"output/dataset_{model_name} {category_name}.md"


def throttle_api_call():
    """
    é™åˆ¶APIè°ƒç”¨é¢‘ç‡ï¼Œç¡®ä¿æ¯åˆ†é’Ÿæœ€å¤šCALL_LIMITæ¬¡è°ƒç”¨ã€‚
    """
    with call_lock:
        current_time = time.time()
        # åªå…³å¿ƒæœ€è¿‘ 60 ç§’å†…çš„è°ƒç”¨æ¬¡æ•°ï¼Œå¦‚æœæœ€æ—©çš„ä¸€æ¬¡è°ƒç”¨å‘ç”Ÿåœ¨ 60 ç§’å‰ï¼Œå°±ä»åˆ—è¡¨é‡Œç§»é™¤å®ƒã€‚
        while call_timestamps and current_time - call_timestamps[0] >= CALL_INTERVAL:
            call_timestamps.pop(0)

        # å¦‚æœå½“å‰è°ƒç”¨æ¬¡æ•°å·²ç»è¾¾åˆ°é™åˆ¶ï¼Œå°±è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´ï¼Œç„¶åä¼‘çœ ã€‚
        if len(call_timestamps) >= CALL_LIMIT:
            sleep_time = CALL_INTERVAL - (current_time - call_timestamps[0])
            print(f"[é™é€Ÿ] ç­‰å¾… {sleep_time:.1f} ç§’ä»¥æ»¡è¶³APIè°ƒç”¨é™åˆ¶")
            time.sleep(sleep_time+1)
            # ç­‰å¾…äº†ä¸€æ®µæ—¶é—´ï¼Œå½“å‰æ—¶é—´å˜äº†ï¼Œæ‰€ä»¥å†æ¬¡æ¸…ç†ä¸€ä¸‹åˆ—è¡¨ä¸­è¿‡æœŸçš„è°ƒç”¨è®°å½•ã€‚
            current_time = time.time()
            while call_timestamps and current_time - call_timestamps[0] >= CALL_INTERVAL:
                call_timestamps.pop(0)

        # æœ€åï¼Œè®°å½•è¿™æ¬¡è°ƒç”¨çš„æ—¶é—´ï¼ŒåŠ å…¥è°ƒç”¨è®°å½•åˆ—è¡¨ä¸­ã€‚
        call_timestamps.append(time.time())

def load_urls(category):
    if os.path.exists(original_urls_file):
        with open(original_urls_file, "rt", encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip()]

    # æ„å»ºæ•°æ®åº“è¿æ¥ä¿¡æ¯
    connection = pymysql.connect(
        host=cfg['db_host'],
        port=cfg['db_port'],
        user=cfg['db_user'],
        password=cfg['db_password'],
        database=cfg['db_name'],
        charset='utf8mb4',
        cursorclass=pymysql.cursors.DictCursor  # è¿”å›å­—å…¸ç±»å‹ç»“æœ
    )

    # SQL è·å–æŸä¸ªåˆ†ç±»çš„æ‰€æœ‰æ–‡ç« é“¾æ¥
    sql = f"""SELECT CONCAT('https://jiayezi.cn/archives/', p.ID) AS post_url
            FROM wp_posts p
            JOIN wp_term_relationships tr ON p.ID = tr.object_id
            JOIN wp_term_taxonomy tt ON tr.term_taxonomy_id = tt.term_taxonomy_id
            JOIN wp_terms t ON tt.term_id = t.term_id
            WHERE p.post_status = 'publish'
              AND p.post_type = 'post'
              AND tt.taxonomy = 'category'
              AND t.name = '{category}'
              order by p.ID
              """
    # æ‰§è¡ŒæŸ¥è¯¢
    with connection:
        with connection.cursor() as cursor:
            cursor.execute(sql)
            result = cursor.fetchall()
            article_urls = [row['post_url'] for row in result]

    # ä¿å­˜åŸå§‹é“¾æ¥åˆ°æ–‡ä»¶
    urls_text = "\n".join(article_urls)
    with open(original_urls_file, "wt", encoding="utf-8") as f:
        f.write(urls_text)

    return article_urls

# è¯»å–å·²å¤„ç†çš„ç½‘å€
def load_processed_urls():
    if not os.path.exists(processed_urls_file):
        return set()
    with open(processed_urls_file, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())

# ä¿å­˜æ–°å¤„ç†çš„ç½‘å€
def save_processed_url(url):
    with open(processed_urls_file, "a", encoding="utf-8") as f:
        f.write(url + "\n")


# ä»ç½‘é¡µæå–æ­£æ–‡å†…å®¹
def extract_article_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        content_div = soup.find("div", class_="entry-content clear")
        if not content_div:
            print(f"[è·³è¿‡] æ‰¾ä¸åˆ°æ­£æ–‡å†…å®¹: {url}")
            return ""

        text = content_div.get_text().strip()
        while "\n\n" in text:
            text = text.replace("\n\n", "\n")
        return text
    except Exception as e:
        print(f"[é”™è¯¯] æå–å¤±è´¥: {url}\nåŸå› : {e}")
        return ""


# å°†æ–‡ç« æ–‡æœ¬å°½é‡å¹³å‡åˆ†æˆå¤šä¸ªç‰‡æ®µï¼Œæ¯ç‰‡æœ€å¤š CHUNK_SIZE å­—
def split_into_chunks(text, max_chars=CHUNK_SIZE):
    total_len = len(text)
    if total_len <= max_chars:
        return [text.strip()]

    # ä¼°ç®—éœ€è¦å¤šå°‘æ®µï¼Œç¡®ä¿æ¯æ®µä¸è¶…è¿‡max_charsã€‚(å…ˆå¯¹æ–‡ç« åˆ†æˆ2ç»„ï¼Œæ£€æŸ¥æ¯ç»„æ˜¯å¦è¶…è¿‡max_charsä¸ªå­—ï¼Œå¦‚æœè¶…è¿‡äº†ï¼Œå°±åˆ†æˆ3ç»„ï¼Œç»§ç»­æ£€æŸ¥ï¼Œä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ°æ¯ç»„ä¸è¶…è¿‡max_charså­—)
    num_chunks = 2
    while total_len / num_chunks > max_chars:
        num_chunks += 1
    approx_len = (total_len // num_chunks) + randint(-50, 50)  # å¢åŠ éšæœºæ€§ï¼Œé¿å…åŒä¸€ç¯‡æ–‡ç« æ¯æ¬¡åˆ†å‰²ç»“æœå®Œå…¨ä¸€æ ·

    paragraphs = [p.strip() for p in text.split("\n") if p.strip()]
    chunks = []
    current = ""

    for i, para in enumerate(paragraphs):
        if len(current) + len(para) + randint(40, 60) <= approx_len:  # æ·»åŠ ä¸€ä¸ªéšæœºå®‰å…¨è¾¹ç•Œï¼Œé¿å…æ®µè½è¿‡é•¿
            current += para + "\n"
        else:
            chunks.append(current.strip())
            current = para + "\n"

            # âœ… æ™ºèƒ½æå‰ç»ˆæ­¢åˆ¤æ–­ï¼ˆæ¯”å¦‚è¦æ‹†åˆ†æˆ3ç»„ï¼Œå¦‚æœchunksåˆ—è¡¨ä¸­å·²ç»æœ‰ä¸¤ç»„æ•°æ®äº†ï¼Œå°±ç›´æ¥æŠŠå‰©ä¸‹çš„æ–‡æœ¬å½“æˆæœ€åä¸€ç»„ï¼Œè¿™ç§åˆ¤æ–­å¯é¿å…åœ¨ç‰¹æ®Šæƒ…å†µä¸‹å¤šæ‹†åˆ†ä¸€æ¬¡ï¼‰
            if len(chunks) == num_chunks - 1:
                # å‰©ä¸‹çš„æ‰€æœ‰æ®µè½åˆå¹¶ä¸ºæœ€åä¸€ç»„
                remaining_text = "\n".join(paragraphs[i+1:]).strip()
                if remaining_text:
                    current += remaining_text
                break

    if current:
        chunks.append(current.strip())

    return chunks


# å‘æ¨¡å‹å‘é€è¯·æ±‚ï¼Œä¿æŒä¸Šä¸‹æ–‡å¯¹è¯
def process_article_chunks(chunks):
    all_output = []
    chat = client.chats.create(model=model_name,
                               config=types.GenerateContentConfig(
        system_instruction=system_prompt,
        temperature=1,
        top_p=1,
        max_output_tokens=4096,
        thinking_config=types.ThinkingConfig(thinking_budget=0)))
    for i, chunk in enumerate(chunks):
        throttle_api_call()
        response = chat.send_message(f"ã€æ–‡ç« ç‰‡æ®µå¼€å§‹ã€‘\n{chunk}\nã€æ–‡ç« ç‰‡æ®µç»“æŸã€‘")
        reply = response.text.strip()
        all_output.append(reply)
    return all_output

# å¤„ç†å•ä¸ªæ–‡ç« é“¾æ¥
def process_single_article(url):
    if url in processed_urls:
        return None
    print(f"[å¤„ç†] æ­£åœ¨å¤„ç†: {url}")
    article_text = extract_article_text(url)
    if not article_text:
        return None
    chunks = split_into_chunks(article_text)
    qa_outputs = process_article_chunks(chunks)
    save_processed_url(url)
    time.sleep(SLEEP_TIME)  # æ¯ç¯‡æ–‡ç« ä¹‹é—´ä¼‘çœ ä¸€æ®µæ—¶é—´ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
    return {"url": url, "qa_outputs": qa_outputs}

# æ‰¹é‡ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶
def save_dataset(urls, output_path, max_workers):
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_single_article, url) for url in urls]

        with open(output_path, "a", encoding="utf-8") as f:
            for future in as_completed(futures):
                result = future.result()
                if not result:
                    continue
                with write_lock:
                    f.write(f"# æ¥æºåœ°å€: {result['url']}\n")
                    for qa in result['qa_outputs']:
                        f.write(qa + "\n\n")
                    f.flush()
    print(f"\nğŸ‰ æ‰€æœ‰æ–‡ç« å¤„ç†å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ°ï¼š{output_path}")


# åˆå§‹åŒ– genai å®¢æˆ·ç«¯
client = genai.Client(api_key=api_key)

# è¯»å–ç³»ç»Ÿæç¤ºè¯
with open('system_prompt.md', "rt", encoding="utf-8") as f:
    system_prompt = f.read().strip()

# è¯·æ±‚èŠ‚æµæ§åˆ¶ï¼ˆæ¯åˆ†é’Ÿæœ€å¤š10æ¬¡ï¼‰
CALL_LIMIT = 10
CALL_INTERVAL = 60  # ç§’
call_timestamps = []
call_lock = threading.Lock()

write_lock = threading.Lock()

processed_urls = load_processed_urls()
article_urls = load_urls(category_name)
print('å·²åŠ è½½åŸå§‹æ–‡ç« é“¾æ¥:', len(article_urls))
save_dataset(article_urls[50:], output_file, max_workers=2)

# å¤„ç†å®Œæ¯•åï¼Œéœ€è¦æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦å‡ºç°â€œä½œè€…â€ã€â€œæ–‡ç« â€ã€â€œæ–‡ä¸­â€ã€â€œæåˆ°â€ã€â€œä»–è®¤ä¸ºâ€ã€â€œèƒŒæ™¯çŸ¥è¯†â€ç­‰å®¢è§‚æè¿°è¯ï¼Œå¦‚æœæœ‰çš„è¯ï¼Œéœ€è¦è½¬æ¢ä¸ºæ›´åˆé€‚çš„æè¿°ã€‚
# è¿˜è¦æ£€æŸ¥é—®å¥ä¸­æ˜¯å¦æœ‰â€œé‚£ä¸ªâ€ã€â€œè¿™äº›â€ç­‰æ¨¡ç³ŠæŒ‡ä»£è¯ï¼Œå¦‚æœæœ‰çš„è¯ï¼Œéœ€è¦è½¬æ¢ä¸ºæ›´æ˜ç¡®çš„æè¿°ã€‚
# è¿˜è¦æ£€æŸ¥é—®å¥ä¸­çš„â€œé—®â€æ˜¯å¦è¢«å†™æˆäº†â€œç­‘/ç­´/é—¯/é—â€è¿™äº›ç¬”ç”»å¤æ‚ã€ç›¸ä¼¼åº¦é«˜çš„å­—ï¼Œå¦åˆ™åœ¨è§£ææ—¶ä¼šå‡ºç°æ··ä¹±ã€‚