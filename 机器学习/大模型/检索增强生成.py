import os
import pickle
import re
import faiss
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
from modelscope.hub.snapshot_download import snapshot_download


def clean_content(raw_text):
    """ æ¸…ç†åŸå§‹æ–‡æœ¬å†…å®¹ï¼Œå»é™¤ä¸å¿…è¦çš„æ ‡è®°å’Œæ ¼å¼"""
    # å»é™¤ "# æ¥æºæ–‡ç« " æ ‡è®°
    raw_text = re.sub(r'# æ¥æºåœ°å€:.*', '', raw_text)
    # å»é™¤ markdown åˆ†å‰²çº¿
    raw_text = re.sub(r'^---$', '', raw_text, flags=re.MULTILINE)
    # å»é™¤åå¼•å·ä»£ç å—æ ‡è®°ï¼ŒåŒ…æ‹¬ ``` å’Œ ```markdown
    raw_text = re.sub(r'```(?:markdown)?\n?', '', raw_text, flags=re.IGNORECASE)
    return raw_text.strip()


def parse_markdown_qa_single_turn(input_file_list):
    """ è§£æ Markdown æ–‡ä»¶ï¼Œæå–é—®ç­”å¯¹ """
    content = ''
    for input_file in input_file_list:
        with open(input_file, 'rt', encoding='utf-8') as f:
            content += f.read() + '\n\n'

    content_cleaned = clean_content(content)

    # æå–æ‰€æœ‰é—®ç­”å¯¹ï¼Œ(question, answer) äºŒå…ƒç»„åˆ—è¡¨
    qa_pairs = re.findall(r'### é—®\s*(.*?)\n+### ç­”\s*(.*?)(?=\n### é—®|\Z)', content_cleaned, re.DOTALL)

    return [f"é—®ï¼š{q.strip()}\nç­”ï¼š{a.strip()}" for q, a in qa_pairs]


# æŒ‡å®šæ•°æ®é›†æ–‡ä»¶
model_name = "gpt-4.1"
input_file_list = [f'output/dataset_{model_name} ç¥è¯.md', f'output/dataset_{model_name} æœ¬è´¨.md', f'output/dataset_{model_name} æ–‡åŒ–.md']

# ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
# æŒ‡å®šæ¨¡å‹åç§°å’Œä¸‹è½½è·¯å¾„
# model_id = 'Qwen/Qwen3-Embedding-0.6B'
# cache_dir = 'D:/models/Qwen3-Embedding-0.6B'
# snapshot_download(model_id, cache_dir=cache_dir)

# æŒ‡å®šæ¨¡å‹è·¯å¾„
# embeddings_name = "Qwen3"
# EMBED_MODEL = "D:/models/Qwen3-Embedding-0.6B"
embeddings_name = "bge-large-zh"
EMBED_MODEL = "D:/models/bge-large-zh"
LLM_MODEL = "D:/models/Qwen3-0.6B"

# å®šä¹‰ç´¢å¼•å’Œå‘é‡çš„å­˜å‚¨è·¯å¾„
INDEX_PATH = f"output/faiss_index_bge_{embeddings_name}.idx"  # å‘é‡ç´¢å¼•ç»“æ„
EMBED_PATH = f"output/doc_embeddings_{embeddings_name}.pkl"  # æ–‡æ¡£å‘é‡
DOC_PATH = f"output/doc_texts_{embeddings_name}.pkl"  # æ–‡æ¡£æ–‡æœ¬

# åŠ è½½ Embedding æ¨¡å‹
print('åŠ è½½ Embedding æ¨¡å‹')
embedding_model = SentenceTransformer(EMBED_MODEL)
# print("embeddingæ¨¡å‹æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆtoken æ•°ï¼‰:", embedding_model.tokenizer.model_max_length)

# åŠ è½½ LLM æ¨¡å‹
print('åŠ è½½ LLM æ¨¡å‹')
tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)
llm = AutoModelForCausalLM.from_pretrained(LLM_MODEL, torch_dtype="auto", device_map="auto")

# å¦‚æœç´¢å¼•å’Œå‘é‡æ–‡ä»¶éƒ½å­˜åœ¨ï¼Œåˆ™ç›´æ¥åŠ è½½
if os.path.exists(INDEX_PATH) and os.path.exists(EMBED_PATH) and os.path.exists(DOC_PATH):
    print("ğŸ“¦ åŠ è½½ç¼“å­˜çš„å‘é‡åº“å’Œç´¢å¼•...")
    # åŠ è½½æ–‡æ¡£å‘é‡
    with open(EMBED_PATH, "rb") as f:
        doc_embeddings = pickle.load(f)
    # åŠ è½½æ–‡æ¡£æ–‡æœ¬
    with open(DOC_PATH, "rb") as f:
        documents = pickle.load(f)
    # åŠ è½½ç´¢å¼•
    index = faiss.read_index(INDEX_PATH)
else:
    print('ğŸ“„ åŠ è½½åŸå§‹æ–‡æ¡£æ•°æ®...')
    documents = parse_markdown_qa_single_turn(input_file_list)
    # documents = [
    #     "â€œè©æâ€åœ¨ä½›æ•™é‡Œæœ‰â€œä¸å†æèµ·â€çš„æ„æ€ï¼Œå› æ­¤â€œè©æç¥–å¸ˆâ€å¯ä»¥ç†è§£ä¸ºâ€œä¸è¦æèµ·ä½ çš„è€å¸ˆâ€æˆ–â€œä¸æç¥–å¸ˆâ€ã€‚å­™æ‚Ÿç©ºçš„å¸ˆçˆ¶é¢„è§åˆ°ä»–ä¼šé—¯ä¸‹å¤§ç¥¸ï¼Œå› æ­¤ä¸€ç›´å‘Šè¯«ä»–ä¸è¦æåŠè‡ªå·±çš„å¸ˆæ‰¿ï¼Œä»¥å…ç¥¸äº‹ç‰µè¿åˆ°å¸ˆçˆ¶å¤´ä¸Šã€‚",
    #     "æˆ‘è®¤ä¸ºé¾™çš„é€ å‹è±¡å¾ç€â€œé›†å¤§æˆä¹‹ç›¸ï¼Œé›†ç™¾å®¶ä¹‹é•¿â€ï¼Œä»£è¡¨é¾™æ—ä»åœ°çƒåŠ¨ç‰©ä¸­æ±‡é›†ä¼—å¤šä¼˜ç‚¹åçš„é›†ä½“å…±è¯†ã€‚å®ƒæ˜¯ä¸€ç§ç²¾ç¥å›¾è…¾ï¼Œè¡¨è¾¾äº†åŒ…å®¹ä¸‡è±¡ã€èåˆæ™ºæ…§çš„ç†å¿µï¼Œè€Œä¸æ˜¯ä¸€ç§å…·ä½“ç”Ÿç‰©å½¢è±¡ã€‚",
    #     "ç¿…è†€æ˜¯ç²¾çµæ—æœ€é‡è¦çš„èº«ä»½ç‰¹å¾ï¼Œä¸åŒçš„ç¿…è†€è±¡å¾ç€ä»–ä»¬ä¸åŒçš„èƒ½åŠ›ã€èº«ä»½å’Œåœ°ä½ã€‚ç¿…è†€ä¸ä»…å¤–å½¢å„å¼‚ï¼Œè¿˜æ‰¿è½½ç€å¸Œæœ›ã€æ¢¦æƒ³ä¸è´£ä»»ï¼Œæ˜¯ç²¾çµä»¬è‡ªæˆ‘è®¤åŒå’Œè¿½æ±‚çš„é‡è¦è±¡å¾ã€‚",
    #     "æˆ‘è®¤ä¸ºçµé­‚æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ç”¨äºè½¬ç”Ÿï¼Œä¿å­˜å’Œä¼ è¾“äººç±»å®Œæ•´æ„è¯†è®¯æ¯çš„é“å…·ã€‚å®ƒæ˜¯è¿æ¥æ—§è½½ä½“å’Œæ–°è½½ä½“ä¹‹é—´çš„ä¸´æ—¶è½½ä½“ï¼Œå¯ä»¥å¸®åŠ©å°†ç²¾ç¥æ„è¯†ä¿¡æ¯ä»ä¸€ä¸ªèº«ä½“è½¬ç§»åˆ°å¦ä¸€ä¸ªï¼Œå°±åƒä¸¤å°ç”µè„‘ä¹‹é—´ç”¨ä¼˜ç›˜ä¼ é€’æ•°æ®ä¸€æ ·ã€‚",
    # ]
    print(f"âœ… æˆåŠŸåŠ è½½æ®µè½æ•°: {len(documents)}")

    print('ğŸ” å¼€å§‹è®¡ç®—å‘é‡...')
    doc_embeddings = embedding_model.encode(documents, convert_to_tensor=True, normalize_embeddings=True)
    print(f"âœ… æˆåŠŸè®¡ç®—æ®µè½å‘é‡: {doc_embeddings.shape}")

    print('ğŸ“š æ„å»º FAISS ç´¢å¼•...')
    embedding_dim = doc_embeddings.shape[1]
    index = faiss.IndexFlatIP(embedding_dim)
    index.add(doc_embeddings.cpu().numpy())

    # ä¿å­˜å‘é‡å’Œç´¢å¼•
    faiss.write_index(index, INDEX_PATH)
    with open(EMBED_PATH, "wb") as f:
        pickle.dump(doc_embeddings, f)
    with open(DOC_PATH, "wb") as f:
        pickle.dump(documents, f)
    print("ğŸ’¾ å‘é‡å’Œç´¢å¼•å·²ä¿å­˜")

# RAG æŸ¥è¯¢ä¸ç”Ÿæˆ
def rag_query(user_query, top_k=5):
    # å¯¹ç”¨æˆ·é—®é¢˜ç”Ÿæˆå‘é‡
    query_embedding = embedding_model.encode([user_query], normalize_embeddings=True)

    # ä»å‘é‡åº“ä¸­æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£
    scores, indices = index.search(query_embedding, top_k)
    relevant_docs = [documents[i] for i in indices[0]]

    # æ‹¼æ¥æç¤ºè¯
    context = "\n\n".join([f"{i+1}. {doc}" for i, doc in enumerate(relevant_docs)])
    prompt = f"""è¯·ä½ æ‰®æ¼”ä¸€ä¸ªåšå­¦å¤šæ‰çš„å²å­¦å®¶å’Œæ–‡å­¦å®¶ï¼Œå‡è®¾ä½ å†™è¿‡å¾ˆå¤šè®°è½½è¿œå¤å†å²ã€ç¤¾ä¼šç°çŠ¶å’Œæ–‡åŒ–çš„æ–‡ç« ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”è¯»è€…é—®é¢˜ã€‚

ä»¥ä¸‹æ˜¯å¯èƒ½ä¸è¯»è€…é—®é¢˜ç›¸å…³çš„å‚è€ƒä¿¡æ¯ï¼š
{context}

è¯»è€…é—®é¢˜ï¼š{user_query}"""

    # æŸ¥çœ‹å®Œæ•´æç¤ºè¯
    with open('output/prompt.txt', 'wt', encoding='utf-8') as f:
        f.write(prompt)

    # æ„é€ å¯¹è¯æ ¼å¼
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)
    inputs = tokenizer([text], return_tensors="pt").to(llm.device)

    outputs = llm.generate(**inputs, max_new_tokens=1024, do_sample=True, temperature=0.7)
    answer = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return answer.strip()

# ä½¿ç”¨
print('è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥exité€€å‡ºï¼‰ï¼š')
while True:
    query = input("ğŸ‘¦ç”¨æˆ·ï¼š")
    if query.lower() in ["exit", "quit", "é€€å‡º"]:
        break
    response = rag_query(query)
    print("ğŸ’¬ AIï¼š", response)
    print("--------------------------------------------------")
