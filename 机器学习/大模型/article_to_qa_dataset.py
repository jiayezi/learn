import json
import os
from urllib.parse import quote_plus
import requests
from bs4 import BeautifulSoup
import time
from sqlalchemy import create_engine, text
from openai import OpenAI


# å…¨å±€å‚æ•°
CHUNK_SIZE = 800  # æ¯æ®µæœ€å¤š 800 å­—
SLEEP_TIME = 1    # æ¯ç¯‡æ–‡ç« ä¹‹é—´ä¼‘çœ æ—¶é—´
output_file = "dataset.md"
processed_urls_file = "processed_urls.txt"  # å·²å¤„ç†çš„ç½‘å€åˆ—è¡¨


# è¯»å–å·²å¤„ç†çš„ç½‘å€
def load_processed_urls():
    if not os.path.exists(processed_urls_file):
        return set()
    with open(processed_urls_file, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())

# ä¿å­˜æ–°å¤„ç†çš„ç½‘å€
def save_processed_url(url):
    with open(processed_urls_file, "a", encoding="utf-8") as f:
        f.write(url + "\n")


# ä»ç½‘é¡µæå–æ­£æ–‡å†…å®¹
def extract_article_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        content_div = soup.find("div", class_="entry-content clear")
        if not content_div:
            print(f"[è·³è¿‡] æ‰¾ä¸åˆ°æ­£æ–‡å†…å®¹: {url}")
            return ""

        text = content_div.get_text().strip()
        while "\n\n" in text:
            text = text.replace("\n\n", "\n")
        return text
    except Exception as e:
        print(f"[é”™è¯¯] æå–å¤±è´¥: {url}\nåŸå› : {e}")
        return ""


# å°†æ–‡ç« æ–‡æœ¬å°½é‡å¹³å‡åˆ†æˆå¤šä¸ªç‰‡æ®µï¼Œæ¯ç‰‡æœ€å¤š CHUNK_SIZE å­—
def split_into_chunks(text, max_chars=CHUNK_SIZE):
    total_len = len(text)
    if total_len <= max_chars:
        return [text.strip()]

    # ä¼°ç®—éœ€è¦å¤šå°‘æ®µï¼Œç¡®ä¿æ¯æ®µä¸è¶…è¿‡max_charsã€‚(å…ˆå¯¹æ–‡ç« åˆ†æˆ2ç»„ï¼Œæ£€æŸ¥æ¯ç»„æ˜¯å¦è¶…è¿‡800å­—ï¼Œå¦‚æœè¶…è¿‡äº†ï¼Œå°±åˆ†æˆ3ç»„ï¼Œç»§ç»­æ£€æŸ¥ï¼Œç›´åˆ°æ¯ç»„ä¸è¶…è¿‡800å­—)
    num_chunks = 2
    while total_len / num_chunks > max_chars:
        num_chunks += 1
    approx_len = total_len // num_chunks

    paragraphs = [p.strip() for p in text.split("\n") if p.strip()]
    chunks = []
    current = ""

    for i, para in enumerate(paragraphs):
        if len(current) + len(para) + 1 <= approx_len:
            current += para + "\n"
        else:
            chunks.append(current.strip())
            current = para + "\n"

            # âœ… æ™ºèƒ½æå‰ç»ˆæ­¢åˆ¤æ–­ï¼ˆæ¯”å¦‚è¦æ‹†åˆ†æˆ3ç»„ï¼Œå¦‚æœchunksåˆ—è¡¨ä¸­å·²ç»æœ‰ä¸¤ç»„æ•°æ®äº†ï¼Œå°±ç›´æ¥æŠŠå‰©ä¸‹çš„æ–‡æœ¬å½“æˆæœ€åä¸€ç»„ï¼Œè¿™ç§åˆ¤æ–­å¯é¿å…å¤šæ‹†åˆ†ä¸€æ¬¡ï¼‰
            if len(chunks) == num_chunks - 1:
                # å‰©ä¸‹çš„æ‰€æœ‰æ®µè½åˆå¹¶ä¸ºæœ€åä¸€ç»„
                remaining_text = "\n".join(paragraphs[i+1:]).strip()
                if remaining_text:
                    current += remaining_text
                break

    if current:
        chunks.append(current.strip())

    return chunks


# å‘æ¨¡å‹å‘é€è¯·æ±‚ï¼Œä¿æŒä¸Šä¸‹æ–‡å¯¹è¯
def process_article_chunks(chunks):
    messages = [
        {"role": "system", "content": system_prompt}
    ]
    all_output = []
    for i, chunk in enumerate(chunks):
        messages.append({"role": "user", "content": f"ã€æ–‡ç« ç‰‡æ®µå¼€å§‹ã€‘\n{chunk}\nã€æ–‡ç« ç‰‡æ®µç»“æŸã€‘"})
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,
            stream=False,
            temperature = 1.3
        )
        reply = response.choices[0].message.content.strip()
        messages.append({"role": "assistant", "content": reply})
        all_output.append(reply)
    return all_output

# ä¸»å¤„ç†é€»è¾‘
def save_articles(urls, output_path):
    processed = load_processed_urls()
    with open(output_path, "a", encoding="utf-8") as f:
        for idx, url in enumerate(urls, 1):
            if url in processed:
                print(f"[è·³è¿‡] å·²å¤„ç†: {url}")
                continue
            print(f"[{idx}/{len(urls)}] æ­£åœ¨å¤„ç†: {url}")
            article_text = extract_article_text(url)
            if not article_text:
                continue
            chunks = split_into_chunks(article_text)
            qa_outputs = process_article_chunks(chunks)
            f.write(f"# æ¥æºæ–‡ç« : {url}\n")
            for qa in qa_outputs:
                f.write(qa + "\n\n")
            f.flush()  # æ¯ç¯‡å¤„ç†å®Œç«‹å³å°†ç¼“å†²åŒºä¸­çš„æ•°æ®å†™å…¥ç£ç›˜
            save_processed_url(url)
            print(f"âœ… å®Œæˆ: {url}")
            time.sleep(SLEEP_TIME)
    print(f"\nğŸ‰ æ‰€æœ‰æ–‡ç« å¤„ç†å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ°ï¼š{output_path}")



with open('config.json') as f:
    cfg = json.load(f)

# å¯¹å¯†ç è¿›è¡Œ URL ç¼–ç ï¼ˆå¯†ç ä¸­åŒ…å« @ ä¼šå¯¼è‡´è¿æ¥å­—ç¬¦ä¸²è§£æé”™è¯¯ï¼Œå› ä¸º @ åœ¨ URL ä¸­æ˜¯ä¿ç•™å­—ç¬¦ï¼Œç”¨äºåˆ†éš”â€œç”¨æˆ·åâ€å’Œâ€œä¸»æœºåœ°å€â€ã€‚ï¼‰
encoded_password = quote_plus(cfg['db_password'])
# æ„é€ æ•°æ®åº“è¿æ¥ URL
db_url = f"mysql+pymysql://{cfg['db_user']}:{encoded_password}@{cfg['db_host']}:{cfg['db_port']}/{cfg['db_name']}"
# åˆ›å»ºå¼•æ“å¹¶æ‰§è¡Œ SQL
engine = create_engine(db_url)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆDeepSeekï¼‰
api_key = cfg['DEEPSEEK_API_KEY']
client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

# æç¤ºè¯æ¨¡æ¿
system_prompt = """
ä½ æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹åŠ©æ‰‹ï¼Œè´Ÿè´£å°†ç”¨æˆ·æä¾›çš„æ–‡ç« å†…å®¹è½¬åŒ–ä¸ºå¤šä¸ªé«˜è´¨é‡çš„é—®ç­”å¯¹æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒä¸€ä¸ªèƒ½ç†è§£ç”¨æˆ·æ€æƒ³ä½“ç³»çš„å¤§è¯­è¨€æ¨¡å‹ã€‚è½¬æ¢æ—¶è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š

1. è¾“å‡ºä¸º Markdown æ ¼å¼ï¼šæ¯ç»„é—®ç­”ç”¨ ### é—® å’Œ ### ç­” æ ‡è®°æ®µè½å¼€å¤´ã€‚æ‰€æœ‰å†…å®¹ä¿æŒ Markdown é£æ ¼ï¼ˆä¾‹å¦‚ç”¨ *æ–œä½“* æˆ– **åŠ ç²—** è¡¨ç¤ºé‡ç‚¹ï¼Œæˆ–è€…ç”¨ - åˆ†é¡¹åˆ—å‡ºä¿¡æ¯ï¼‰ã€‚æ¯ç»„é—®ç­”ç»“æ„å¦‚ä¸‹ï¼š
### é—®
ï¼ˆæé—®å†…å®¹ï¼‰

### ç­”
ï¼ˆå›ç­”å†…å®¹ï¼‰

2. æ¯ç»„é—®ç­”éœ€æ¶µç›–æ–‡ç« ä¸­çš„ä¸€ä¸ªç‹¬ç«‹æ¦‚å¿µã€ä¿¡æ¯ç‚¹æˆ–è®¾å®šï¼Œå†…å®¹å¯†åº¦é«˜çš„æ®µè½å¯æ‹†åˆ†ä¸ºå¤šç»„é—®ç­”ï¼Œç¡®ä¿è¦†ç›–å…¨éƒ¨å…³é”®ä¿¡æ¯ã€‚

3. æœ€å¥½ä½¿ç”¨ç¬¬äºŒäººç§°â€œä½ â€æé—®ï¼Œå›ç­”ä½¿ç”¨ç¬¬ä¸€äººç§°â€œæˆ‘â€ä½œç­”ï¼Œé€‚å½“ä¿ç•™åŸæ–‡çš„è¯´è¯é£æ ¼ã€‚

4. é—®å¥è¦è‡ªç„¶ã€å…·ä½“ã€èšç„¦ï¼ŒæŒ‡å‘æ¸…æ™°ï¼›ç­”æ¡ˆè¦å°½å¯èƒ½å……å®ã€é€šé¡ºã€è‡ªç„¶ã€å®Œæ•´ã€å‡†ç¡®ã€ç¬¦åˆäººç±»å¸¸è§„è¡¨è¾¾ä¹ æƒ¯ï¼Œé€‚å½“å±•å¼€è¡¨è¾¾ï¼Œä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ï¼Œé¿å…ç®€ç•¥çš„å›åº”ã€‚åœ¨ä¸æ”¹å˜åŸæ„çš„å‰æä¸‹ï¼Œå¯ä»¥é€‚å½“è¡¥å……ä¸Šä¸‹æ–‡ã€æ‰©å±•é€»è¾‘é“¾æ¡æˆ–åŠ æ·±è§£é‡Šï¼Œè®©ç­”æ¡ˆå†…å®¹æ›´åŠ å®Œæ•´é¥±æ»¡ã€‚

5. é—®å¥ä¸ç­”æ¡ˆéœ€é«˜åº¦åŒ¹é…ï¼Œç¡®ä¿è¯­ä¹‰è¡”æ¥ç´§å¯†ï¼Œæœ‰åˆ©äºè¯­è¨€æ¨¡å‹å­¦ä¹ æ­£ç¡®çš„æé—®-å›ç­”å¯¹é½æ¨¡å¼ã€‚

6. ä¸è¦æ›´æ”¹åŸæ„ã€å‰Šå¼±åŸæ–‡è§‚ç‚¹æˆ–å¼•å…¥å¤–éƒ¨è¯„ä»·ï¼›æ‰€æœ‰é—®ç­”å‡åŸºäºåŸæ–‡å†…å®¹æå–ç”Ÿæˆã€‚

è¯·ä½ ç°åœ¨å¤„ç†ä»¥ä¸‹è¿™ç¯‡æ–‡ç« çš„ä¸€éƒ¨åˆ†å†…å®¹ï¼ˆå¦‚ä¸‹ï¼‰ï¼Œå¹¶æŒ‰ç…§ä¸Šè¿°è¦æ±‚æå–é«˜è´¨é‡é—®ç­”æ•°æ®ã€‚
è¯·ä¸“æ³¨äºå½“å‰æä¾›çš„å†…å®¹ç‰‡æ®µï¼Œå®Œæ•´æå–å…¶ä¸­çš„ç‹¬ç«‹æ¦‚å¿µã€ä¿¡æ¯ç‚¹æˆ–è®¾å®šï¼Œå°½å¯èƒ½è¦†ç›–å…¨éƒ¨å…³é”®ä¿¡æ¯ã€‚
æ— éœ€ç­‰å¾…å…¨æ–‡æä¾›ï¼Œæ¯æ¬¡ç”¨æˆ·å°†ç»§ç»­å‘é€ä¸‹ä¸€éƒ¨åˆ†å†…å®¹ï¼Œè¯·ä¿æŒå¤„ç†é£æ ¼ä¸€è‡´ã€‚
"""

# SQL è·å–æŸä¸ªåˆ†ç±»çš„æ‰€æœ‰æ–‡ç« é“¾æ¥
sql = """SELECT CONCAT('https://jiayezi.cn/archives/', p.ID) AS post_url
        FROM wp_posts p
        JOIN wp_term_relationships tr ON p.ID = tr.object_id
        JOIN wp_term_taxonomy tt ON tr.term_taxonomy_id = tt.term_taxonomy_id
        JOIN wp_terms t ON tt.term_id = t.term_id
        WHERE p.post_status = 'publish'
          AND p.post_type = 'post'
          AND tt.taxonomy = 'category'
          AND t.name = 'ç¥è¯'"""

# æ‰§è¡ŒæŸ¥è¯¢
with engine.connect() as conn:
    result = conn.execute(text(sql))
    article_urls = [row['post_url'] for row in result.mappings()]

save_articles(article_urls, output_file)
